{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline de Fine-Tuning de LLMs com QLoRA\n",
        "\n",
        "## MLOps com Google Colab + DagsHub/MLflow\n",
        "\n",
        "- Google Colab T4 GPU (Gratuito)\n",
        "- Qwen 2.5 (1.5B) - Open Source\n",
        "- QLoRA + bitsandbytes (4-bit)\n",
        "- DagsHub + MLflow (Free Tier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Instalacao de Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch transformers datasets accelerate peft bitsandbytes trl mlflow dagshub huggingface_hub sentencepiece protobuf\n",
        "print(\"[OK] Dependencias instaladas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"CUDA disponivel: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memoria: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, warnings\n",
        "from datetime import datetime\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "import mlflow, dagshub\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"[OK] Imports realizados!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuracao de Credenciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DAGSHUB_USERNAME = \"seu_username\"\n",
        "DAGSHUB_REPO = \"seu_repositorio\"\n",
        "DAGSHUB_TOKEN = \"seu_token\"\n",
        "os.environ[\"DAGSHUB_USERNAME\"] = DAGSHUB_USERNAME\n",
        "os.environ[\"DAGSHUB_TOKEN\"] = DAGSHUB_TOKEN\n",
        "print(\"[OK] Credenciais configuradas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuracoes do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen2.5-1.5B\"\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "print(f\"[OK] Modelo: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"[INFO] Carregando modelo...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(f\"[OK] Modelo carregado! Memoria: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dataset de Exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLE_DATA = [\n",
        "    {\"instruction\": \"O que e Machine Learning?\", \"response\": \"Machine Learning e uma area da IA que permite computadores aprenderem padroes.\"},\n",
        "    {\"instruction\": \"O que e Fine-Tuning?\", \"response\": \"Fine-Tuning e ajustar um modelo pre-treinado para uma tarefa especifica.\"},\n",
        "    {\"instruction\": \"O que e LoRA?\", \"response\": \"LoRA e uma tecnica eficiente de fine-tuning que adiciona pequenas matrizes treinaveis.\"},\n",
        "    {\"instruction\": \"O que e Quantizacao?\", \"response\": \"Quantizacao reduz a precisao dos pesos do modelo para economizar memoria.\"},\n",
        "    {\"instruction\": \"O que e MLOps?\", \"response\": \"MLOps combina ML com praticas DevOps para tornar ML reproduzivel e escalavel.\"},\n",
        "]\n",
        "dataset = Dataset.from_list(SAMPLE_DATA)\n",
        "print(f\"[OK] Dataset com {len(dataset)} amostras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatar_prompt(ex):\n",
        "    return f\"### Instrucao:\\n{ex['instruction']}\\n\\n### Resposta:\\n{ex['response']}\"\n",
        "\n",
        "def tokenize(examples):\n",
        "    texts = [formatar_prompt({\"instruction\": i, \"response\": r}) for i, r in zip(examples[\"instruction\"], examples[\"response\"])]\n",
        "    tok = tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")\n",
        "    tok[\"labels\"] = tok[\"input_ids\"].copy()\n",
        "    return tok\n",
        "\n",
        "train_dataset = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
        "print(\"[OK] Dataset tokenizado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configuracao LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"[OK] LoRA aplicado! Treinaveis: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    args=training_args,\n",
        "    max_seq_length=512,\n",
        ")\n",
        "print(\"[OK] Trainer configurado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "print(\"[INFO] Iniciando treinamento...\")\n",
        "start = time.time()\n",
        "result = trainer.train()\n",
        "print(f\"[OK] Treinamento concluido em {(time.time()-start)/60:.2f} minutos\")\n",
        "print(f\"Loss final: {result.metrics['train_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Salvamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = os.path.join(OUTPUT_DIR, \"fine-tuned-model\")\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(f\"[OK] Modelo salvo em: {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Teste de Inferencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gerar(instrucao):\n",
        "    prompt = f\"### Instrucao:\\n{instrucao}\\n\\n### Resposta:\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=256, temperature=0.7, top_p=0.9, pad_token_id=tokenizer.eos_token_id)\n",
        "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    return text.split(\"### Resposta:\")[-1].strip() if \"### Resposta:\" in text else text\n",
        "\n",
        "print(\"Teste: O que e Deep Learning?\")\n",
        "print(f\"Resposta: {gerar('O que e Deep Learning?')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concluido!\n",
        "\n",
        "Pipeline de Fine-Tuning MLOps executado com sucesso."
      ]
    }
  ]
}