{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline de Fine-Tuning de LLMs com QLoRA\n",
        "\n",
        "## MLOps com Google Colab + DagsHub/MLflow\n",
        "\n",
        "| Componente | Tecnologia | Custo |\n",
        "|------------|------------|-------|\n",
        "| **Compute** | Google Colab T4 GPU (16GB VRAM) | Gratuito |\n",
        "| **Modelo** | LLaMA 3.2 (3B Instruct) | Open Source |\n",
        "| **Dataset** | Guanaco 1K | Open Source |\n",
        "| **Otimizacao** | QLoRA + bitsandbytes (4-bit) | Open Source |\n",
        "| **Tracking** | DagsHub + MLflow | Free Tier |\n",
        "\n",
        "> **IMPORTANTE**: LLaMA 3 requer autenticacao no Hugging Face:\n",
        "> 1. Criar conta em huggingface.co\n",
        "> 2. Aceitar os termos em https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
        "> 3. Gerar um token de acesso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Instalacao de Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q torch transformers>=4.40.0 datasets>=2.14.0 accelerate>=0.24.0 peft>=0.6.0 bitsandbytes>=0.41.0 trl>=0.7.0 mlflow>=2.8.0 dagshub>=0.3.0 huggingface_hub>=0.19.0 sentencepiece protobuf\n",
        "print(\"[OK] Dependencias instaladas!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verificacao de GPU e Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import os\n",
        "import warnings\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset, load_dataset\n",
        "import mlflow\n",
        "import dagshub\n",
        "from huggingface_hub import login\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"[OK] GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
        "else:\n",
        "    raise RuntimeError(\"GPU nao disponivel! Va em Runtime -> Change runtime type -> GPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Autenticacao Hugging Face (Obrigatorio para LLaMA 3)\n",
        "\n",
        "**Passos:**\n",
        "1. Acesse https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
        "2. Aceite os termos de uso\n",
        "3. Va em Settings -> Access Tokens -> New Token\n",
        "4. Cole o token abaixo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Opcao 1: Cole seu token diretamente (menos seguro)\n",
        "HF_TOKEN = \"seu_token_aqui\"\n",
        "\n",
        "# Opcao 2: Use secrets do Colab (mais seguro)\n",
        "# from google.colab import userdata\n",
        "# HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "print(\"[OK] Autenticado no Hugging Face!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configuracoes do Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Modelo LLaMA 3\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "# Dataset do Hugging Face\n",
        "DATASET_NAME = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# Quantizacao 4-bit\n",
        "QUANTIZATION_CONFIG = {\n",
        "    \"load_in_4bit\": True,\n",
        "    \"bnb_4bit_compute_dtype\": torch.float16,\n",
        "    \"bnb_4bit_quant_type\": \"nf4\",\n",
        "    \"bnb_4bit_use_double_quant\": True,\n",
        "}\n",
        "\n",
        "# LoRA\n",
        "LORA_CONFIG = {\n",
        "    \"r\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"bias\": \"none\",\n",
        "    \"task_type\": \"CAUSAL_LM\",\n",
        "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "}\n",
        "\n",
        "# Treinamento\n",
        "TRAINING_CONFIG = {\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"per_device_train_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"warmup_ratio\": 0.03,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"max_seq_length\": 1024,\n",
        "    \"logging_steps\": 10,\n",
        "    \"save_steps\": 50,\n",
        "}\n",
        "\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "\n",
        "print(f\"[OK] Modelo: {MODEL_NAME}\")\n",
        "print(f\"[OK] Dataset: {DATASET_NAME}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. MLflow com DagsHub (Opcional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DAGSHUB_USERNAME = \"seu_username\"\n",
        "DAGSHUB_REPO = \"seu_repositorio\"\n",
        "DAGSHUB_TOKEN = \"seu_token\"\n",
        "\n",
        "def setup_mlflow():\n",
        "    dagshub.init(repo_name=DAGSHUB_REPO, repo_owner=DAGSHUB_USERNAME, mlflow=True)\n",
        "    tracking_uri = f\"https://dagshub.com/{DAGSHUB_USERNAME}/{DAGSHUB_REPO}.mlflow\"\n",
        "    mlflow.set_tracking_uri(tracking_uri)\n",
        "    os.environ[\"MLFLOW_TRACKING_USERNAME\"] = DAGSHUB_USERNAME\n",
        "    os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = DAGSHUB_TOKEN\n",
        "    mlflow.set_experiment(\"llama3-fine-tuning\")\n",
        "    print(f\"[OK] MLflow: {tracking_uri}\")\n",
        "\n",
        "# Descomente para ativar\n",
        "# setup_mlflow()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Carregamento do Modelo (4-bit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*60)\n",
        "print(f\"Carregando modelo: {MODEL_NAME}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=QUANTIZATION_CONFIG[\"load_in_4bit\"],\n",
        "    bnb_4bit_compute_dtype=QUANTIZATION_CONFIG[\"bnb_4bit_compute_dtype\"],\n",
        "    bnb_4bit_quant_type=QUANTIZATION_CONFIG[\"bnb_4bit_quant_type\"],\n",
        "    bnb_4bit_use_double_quant=QUANTIZATION_CONFIG[\"bnb_4bit_use_double_quant\"],\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "memory_gb = model.get_memory_footprint() / (1024**3)\n",
        "print(f\"[OK] Modelo carregado! Memoria: {memory_gb:.2f} GB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Carregamento do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Carregando dataset: {DATASET_NAME}\")\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "print(f\"[OK] {len(dataset)} amostras\")\n",
        "print(f\"[EXEMPLO]: {dataset[0]['text'][:300]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Aplicacao do LoRA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=LORA_CONFIG[\"r\"],\n",
        "    lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
        "    lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
        "    bias=LORA_CONFIG[\"bias\"],\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"[OK] LoRA: {trainable:,} treinaveis ({100*trainable/total:.2f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=TRAINING_CONFIG[\"num_train_epochs\"],\n",
        "    per_device_train_batch_size=TRAINING_CONFIG[\"per_device_train_batch_size\"],\n",
        "    gradient_accumulation_steps=TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
        "    learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
        "    warmup_ratio=TRAINING_CONFIG[\"warmup_ratio\"],\n",
        "    lr_scheduler_type=TRAINING_CONFIG[\"lr_scheduler_type\"],\n",
        "    logging_steps=TRAINING_CONFIG[\"logging_steps\"],\n",
        "    save_steps=TRAINING_CONFIG[\"save_steps\"],\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=TRAINING_CONFIG[\"max_seq_length\"],\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"[OK] Trainer pronto!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"INICIANDO TREINAMENTO...\")\n",
        "trainer.train()\n",
        "print(\"[OK] Treinamento concluido!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Salvamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_path = f\"{OUTPUT_DIR}/llama3-finetuned\"\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(f\"[OK] Salvo em: {model_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Teste de Inferencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gerar_resposta(prompt, max_tokens=256):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=tokenizer.eos_token_id)\n",
        "    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "print(\"TESTE:\")\n",
        "for q in [\"O que e Machine Learning?\", \"Explique fine-tuning em LLMs.\"]:\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {gerar_resposta(q)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}