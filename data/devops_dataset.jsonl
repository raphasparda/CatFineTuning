{"instruction": "Como criar um teste de integracao para uma API REST usando pytest?", "response": "Para criar um teste de integracao com pytest, primeiro instale as dependencias necessarias: pip install pytest requests pytest-mock. Crie um arquivo test_api.py e utilize o fixture do pytest para configurar o cliente. Use requests para fazer chamadas reais a API e assertions para validar os responses. Exemplo basico: import pytest, requests. def test_get_users(): response = requests.get('http://api.exemplo.com/users'); assert response.status_code == 200; assert 'users' in response.json(). Para ambientes de CI/CD, considere usar variaveis de ambiente para URLs e credenciais."}
{"instruction": "Qual a diferenca entre testes unitarios e testes de integracao no contexto de DevOps?", "response": "Testes unitarios verificam componentes isolados do codigo, como funcoes ou metodos individuais, usando mocks para dependencias externas. Sao rapidos, executam em milissegundos e devem cobrir a maior parte do codigo. Testes de integracao validam a comunicacao entre componentes reais, como banco de dados, APIs externas ou microservicos. Sao mais lentos mas garantem que o sistema funciona como um todo. No pipeline de CI/CD, testes unitarios rodam primeiro, seguidos dos de integracao. A piramide de testes sugere mais unitarios na base, menos integracao no meio e poucos testes end-to-end no topo."}
{"instruction": "Como configurar GitHub Actions para rodar testes automatizados em cada pull request?", "response": "Crie o arquivo .github/workflows/tests.yml com o seguinte conteudo: name: Tests, on: [pull_request], jobs: test: runs-on: ubuntu-latest, steps: - uses: actions/checkout@v4, - name: Setup Python uses: actions/setup-python@v4 with: python-version: '3.11', - name: Install dependencies run: pip install -r requirements.txt, - name: Run tests run: pytest tests/ -v --cov=src --cov-report=xml, - name: Upload coverage uses: codecov/codecov-action@v3. Isso executa testes em cada PR, gera relatorio de cobertura e falha o PR se os testes nao passarem."}
{"instruction": "Explique como implementar testes de carga com Locust para validar escalabilidade.", "response": "Locust e uma ferramenta de teste de carga em Python. Instale com pip install locust. Crie um arquivo locustfile.py definindo o comportamento dos usuarios: from locust import HttpUser, task, between. class WebsiteUser(HttpUser): wait_time = between(1, 3), @task def load_homepage(self): self.client.get('/'), @task(3) def load_api(self): self.client.get('/api/data'). O decorator @task(3) indica que essa tarefa tem 3x mais chance de ser executada. Execute com: locust -f locustfile.py --host=http://sua-api.com. Acesse localhost:8089 para configurar numero de usuarios e taxa de spawn. Analise metricas como RPS, latencia p95 e taxa de erro."}
{"instruction": "Como escrever testes para containers Docker usando testcontainers?", "response": "Testcontainers permite criar containers descartaveis para testes. Instale: pip install testcontainers. Exemplo para testar com PostgreSQL: from testcontainers.postgres import PostgresContainer. def test_database(): with PostgresContainer('postgres:15') as postgres: engine = create_engine(postgres.get_connection_url()), with engine.connect() as conn: result = conn.execute(text('SELECT 1')), assert result.scalar() == 1. O container e criado antes do teste e destruido apos. Funciona com MySQL, Redis, Kafka, Elasticsearch e outros. Ideal para testes de integracao em CI/CD pois garante ambiente limpo e reproduzivel."}
{"instruction": "Qual a melhor estrategia para implementar testes de contrato entre microservicos?", "response": "Testes de contrato garantem que a comunicacao entre servicos segue um acordo pre-definido. Use Pact para implementar Consumer-Driven Contracts. O consumidor define suas expectativas, gerando um contrato JSON. O provedor valida esse contrato em seus testes. Fluxo: 1) Consumidor escreve teste com expectativas, 2) Pact gera arquivo de contrato, 3) Contrato e publicado no Pact Broker, 4) Pipeline do provedor baixa contratos e valida, 5) Resultado e reportado ao broker. Isso evita quebras de integracao e permite deploy independente. Alternativas incluem Spring Cloud Contract para Java e Dredd para APIs com OpenAPI."}
{"instruction": "Como implementar testes de seguranca automatizados no pipeline de CI/CD?", "response": "Integre ferramentas de seguranca em diferentes estagios. SAST com Bandit para Python ou SonarQube para analise estatica do codigo. DAST com OWASP ZAP para testar aplicacao em execucao. Dependency scanning com Safety ou Snyk para vulnerabilidades em bibliotecas. Container scanning com Trivy para imagens Docker. Secrets detection com git-secrets ou truffleHog. Exemplo no GitHub Actions: - name: Security scan run: bandit -r src/ -f json -o bandit-report.json, - name: Dependency check run: safety check -r requirements.txt. Configure para falhar o pipeline em vulnerabilidades criticas e alertar em medias."}
{"instruction": "Explique como criar testes de infraestrutura com Terraform e Terratest.", "response": "Terratest e uma biblioteca Go para testar infraestrutura como codigo. Valida que recursos Terraform foram criados corretamente. Instale Go e execute: go get github.com/gruntwork-io/terratest. Crie test/terraform_test.go: func TestTerraform(t *testing.T) { opts := terraform.Options{TerraformDir: '../terraform'}, defer terraform.Destroy(t, opts), terraform.InitAndApply(t, opts), output := terraform.Output(t, opts, 'instance_ip'), assert.NotEmpty(t, output). O teste aplica a infraestrutura real, valida outputs e destroi apos. Para AWS, use aws-sdk-go para verificar recursos. Execute em ambiente isolado ou use LocalStack para simular AWS localmente."}
{"instruction": "Como configurar testes de smoke em um deployment Kubernetes?", "response": "Testes de smoke validam funcionalidades basicas apos deploy. Crie um Job Kubernetes que executa apos o Deployment: apiVersion: batch/v1, kind: Job, metadata: name: smoke-test, spec: template: spec: containers: - name: test, image: curlimages/curl, command: ['sh', '-c', 'curl -f http://myapp-service/health || exit 1'], restartPolicy: Never, backoffLimit: 3. Use Helm hooks para executar automaticamente: annotations: helm.sh/hook: post-install,post-upgrade. Alternativa com kubectl: kubectl wait --for=condition=ready pod -l app=myapp --timeout=60s && kubectl exec -it test-pod -- curl http://myapp/health. Integre com ArgoCD ou Flux para rollback automatico se falhar."}
{"instruction": "Qual a diferenca entre mocks, stubs e fakes em testes automatizados?", "response": "Mocks sao objetos que verificam comportamento, registram chamadas recebidas e permitem assertions sobre como foram usados. Exemplo: mock.assert_called_once_with('param'). Stubs fornecem respostas pre-programadas para chamadas, sem verificacao de comportamento. Retornam dados fixos independente da entrada. Fakes sao implementacoes funcionais simplificadas, como um banco de dados em memoria ao inves do real. Dummies sao objetos passados mas nunca usados, apenas preenchem parametros. Use mocks quando importa como o objeto foi usado, stubs quando precisa apenas de retornos controlados, e fakes quando precisa de comportamento real simplificado."}
{"instruction": "Como implementar testes de regressao visual com Percy ou Chromatic?", "response": "Testes de regressao visual capturam screenshots e comparam com baseline. Percy integra com Cypress: cy.percySnapshot('Homepage'). Cada commit gera screenshots comparadas automaticamente. Diferencas visuais aparecem para aprovacao manual. Chromatic e especifico para Storybook: npx chromatic --project-token=xxx. Captura cada story como screenshot. Fluxo: 1) Defina baseline em main, 2) PRs geram novas capturas, 3) Diferencas sao destacadas, 4) Aprove ou rejeite mudancas. Configure no CI: - name: Visual tests run: npx percy exec -- cypress run. Custos variam por numero de snapshots. Alternativa open source: BackstopJS para comparacao local."}
{"instruction": "Explique como criar testes de performance com k6 integrados ao pipeline.", "response": "k6 e uma ferramenta de teste de carga moderna. Instale e crie script.js: import http from 'k6/http', import { check, sleep } from 'k6'. export const options = { vus: 50, duration: '30s', thresholds: { http_req_duration: ['p(95)<500'] } }. export default function() { const res = http.get('http://api.com/endpoint'), check(res, { 'status 200': (r) => r.status === 200 }), sleep(1) }. Execute: k6 run script.js. No GitHub Actions: - uses: grafana/k6-action@v0.3.0 with: filename: tests/load/script.js. Thresholds definem criterios de falha. Integre com Grafana Cloud k6 para historico e dashboards. Exporte metricas para Prometheus com --out prometheus."}
{"instruction": "Como testar lambdas AWS localmente antes do deploy?", "response": "Use AWS SAM CLI para testar localmente. Instale sam-cli e configure template.yaml definindo a funcao. Execute sam local invoke FunctionName -e event.json para testar com evento especifico. Para APIs: sam local start-api inicia servidor local na porta 3000. Debug com sam local invoke -d 5858 para attach de debugger. Para testes unitarios, use pytest com moto para mockar servicos AWS: from moto import mock_s3. @mock_s3 def test_s3_upload(): conn = boto3.resource('s3'), conn.create_bucket(Bucket='test'), ... Localstack e alternativa que simula varios servicos AWS. Configure AWS_ENDPOINT_URL para apontar para localstack em testes de integracao."}
{"instruction": "Qual a importancia de testes de acessibilidade e como automatiza-los?", "response": "Testes de acessibilidade garantem que a aplicacao e usavel por pessoas com deficiencias. Automatize com axe-core integrado ao Cypress: cy.injectAxe(), cy.checkA11y(). Ou com Pa11y no CI: pa11y http://localhost:3000 --reporter json. WAVE API permite validacao programatica. Playwright tem suporte nativo: await expect(page).toHaveNoViolations(). Valide contraste de cores, textos alternativos, navegacao por teclado e estrutura de headings. WCAG 2.1 define niveis A, AA e AAA. Foque em AA para conformidade legal. Inclua no pipeline: - name: Accessibility run: npm run test:a11y. Ferramentas nao substituem testes manuais com leitores de tela, mas capturam problemas comuns automaticamente."}
{"instruction": "Como implementar chaos engineering com Chaos Monkey ou Litmus?", "response": "Chaos engineering testa resiliencia injetando falhas controladas. Chaos Monkey da Netflix mata instancias aleatoriamente em producao. Litmus e mais configuravel para Kubernetes. Instale: kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator.yaml. Crie experimento: apiVersion: litmuschaos.io/v1alpha1, kind: ChaosEngine, metadata: name: nginx-chaos, spec: appinfo: appns: default, applabel: app=nginx, chaosServiceAccount: litmus-admin, experiments: - name: pod-delete. Isso deleta pods aleatoriamente testando recuperacao. Monitore metricas durante o experimento. Comece em staging, depois producao em horarios de baixo trafego. Gremlin e alternativa comercial com UI amigavel. Documente hipoteses e resultados de cada experimento."}
{"instruction": "Explique estrategias de teste para pipelines de dados e ETL.", "response": "Testes de pipelines de dados incluem: 1) Validacao de schema com Great Expectations ou Pandera, 2) Testes de qualidade verificando nulls, duplicatas e ranges, 3) Testes de reconciliacao comparando contagens fonte vs destino, 4) Testes de idempotencia executando pipeline multiplas vezes, 5) Testes de performance com volumes realistas. Great Expectations exemplo: expect_column_values_to_not_be_null('user_id'), expect_column_values_to_be_between('age', 0, 120). Gere data profiles para baseline. dbt inclui testes nativos: models/schema.yml define testes de unicidade e relacionamentos. Para Airflow, use dag.test() para validar DAGs sem executar. Pytest com fixtures para dados de teste isolados."}
{"instruction": "Como configurar testes de API com Postman/Newman no CI/CD?", "response": "Exporte colecao Postman como JSON. Newman executa colecoes via CLI: newman run collection.json -e environment.json --reporters cli,junit --reporter-junit-export results.xml. No GitHub Actions: - name: Run API tests run: | npm install -g newman, newman run tests/api/collection.json -e tests/api/env.json --reporters cli,htmlextra --reporter-htmlextra-export report.html. Use pre-request scripts para setup dinamico e tests para validacoes. Variaveis de ambiente separam configs por ambiente. Collection runner permite execucao sequencial ou paralela. Postman monitors executam testes periodicamente. Alternativas: REST Assured para Java, httpx com pytest para Python, ou Karate para BDD-style."}
{"instruction": "Qual a melhor forma de gerenciar dados de teste em ambientes de CI/CD?", "response": "Estrategias para dados de teste: 1) Fixtures com dados estaticos em arquivos JSON/YAML, versionados com codigo. 2) Factory pattern com bibliotecas como factory_boy ou Faker para gerar dados dinamicos. 3) Database seeding com scripts SQL ou migrations dedicadas. 4) Snapshots do banco de producao anonimizados para testes realistas. 5) Containers com dados pre-carregados para consistencia. Evite dependencia de dados de producao diretamente. Use transactions em testes para rollback automatico. Exemplo factory_boy: class UserFactory(factory.Factory): class Meta: model = User, name = factory.Faker('name'), email = factory.Faker('email'). Reset banco entre suites de teste. Considere banco in-memory como SQLite para velocidade em testes unitarios."}
{"instruction": "Como implementar testes de mutacao para avaliar qualidade dos testes?", "response": "Testes de mutacao modificam o codigo fonte e verificam se testes detectam a mudanca. Mutpy para Python: pip install mutmut. Execute: mutmut run --paths-to-mutate=src/. Mutacoes incluem: trocar operadores (+ por -), inverter condicionais, remover linhas. Se testes passam com mutante, significa cobertura insuficiente. Relatorio mostra mutation score: mutantes mortos / total. Score acima de 80% indica testes robustos. Pitest para Java, Stryker para JavaScript. E computacionalmente caro, execute em subset do codigo ou periodicamente. Configure no CI para rodar em PRs que tocam arquivos criticos. Foque em codigo de negocio, ignore boilerplate. Complementa metricas de cobertura tradicionais."}
{"instruction": "Explique como criar testes end-to-end com Playwright para aplicacoes web.", "response": "Playwright testa em multiplos browsers com API moderna. Instale: pip install playwright && playwright install. Crie test_app.py: from playwright.sync_api import sync_playwright. def test_login(): with sync_playwright() as p: browser = p.chromium.launch(), page = browser.new_page(), page.goto('http://localhost:3000'), page.fill('[name=email]', 'user@test.com'), page.fill('[name=password]', 'senha123'), page.click('button[type=submit]'), assert page.url == 'http://localhost:3000/dashboard', browser.close(). Use pytest-playwright para integracao: pytest --browser chromium --browser firefox. Page Object Model organiza codigo. Trace viewer ajuda debugging: page.tracing.start(screenshots=True). CI config: xvfb-run pytest para browsers headless em Linux."}
{"instruction": "Como configurar cobertura de codigo e gates de qualidade no pipeline?", "response": "Configure pytest-cov para cobertura: pytest --cov=src --cov-report=xml --cov-fail-under=80. O parametro --cov-fail-under falha se cobertura for menor que 80%. Integre com Codecov ou Coveralls para tracking historico. SonarQube define quality gates compostos: cobertura minima, duplicacao maxima, bugs e vulnerabilidades zero. GitHub Actions exemplo: - name: Test with coverage run: pytest --cov --cov-report=xml, - uses: codecov/codecov-action@v3. Configure branch protection rules exigindo checks passarem. .coveragerc personaliza exclusoes: [run] omit = tests/*, */__init__.py. Cobertura de branches (--cov-branch) e mais rigorosa que linhas. Foque em codigo critico, 100% nem sempre e necessario ou pratico."}
{"instruction": "Qual a importancia de testes de configuracao e como implementa-los?", "response": "Testes de configuracao validam que ambientes estao corretos antes do deploy. Serverspec ou Inspec verificam estado de servidores: describe package('nginx') do it { should be_installed } end. Para Kubernetes, use conftest com OPA: conftest test deployment.yaml verifica policies como limites de recursos obrigatorios. Teste variaveis de ambiente necessarias, conexoes com servicos externos, permissoes de arquivos e portas abertas. Pre-deploy checks: curl health endpoints, validar secrets existem, verificar quotas de recursos. Ansible tem ansible-lint e molecule para testar roles. Terraform validate e tflint para IaC. Chef tem ChefSpec e Test Kitchen. Documente dependencias de configuracao e automatize validacao no pipeline antes de qualquer deploy."}
{"instruction": "Como testar webhooks e integracoes assincronas?", "response": "Webhooks sao dificeis de testar por natureza assincrona. Estrategias: 1) Mock do endpoint receptor em testes unitarios. 2) Use ngrok ou smee.io para expor localhost e receber webhooks reais em desenvolvimento. 3) Para CI, crie servidor mock com flask que registra chamadas recebidas. 4) Teste producer e consumer separadamente: producer valida payload correto, consumer valida processamento. 5) Contract tests garantem formato acordado. 6) Para eventos em filas, use containers de RabbitMQ/Kafka em testes. Exemplo com responses: @responses.activate def test_webhook(): responses.add(responses.POST, 'http://webhook.com', status=200), send_webhook(data), assert len(responses.calls) == 1. Valide retries, idempotencia e tratamento de falhas."}
{"instruction": "Explique como implementar testes de banco de dados e migrations.", "response": "Testes de migrations garantem evolucao segura do schema. Alembic para Python: alembic upgrade head && alembic downgrade base && alembic upgrade head valida ciclo completo. Teste cada migration individualmente em CI. Para dados, use transacoes com rollback: @pytest.fixture def db_session(): connection = engine.connect(), transaction = connection.begin(), session = Session(bind=connection), yield session, transaction.rollback(). Teste constraints, indexes e triggers com queries especificas. Valide performance de queries criticas com EXPLAIN ANALYZE. Use banco dedicado para testes, nunca producao. Docker compose sobe banco limpo para cada suite. Para dados sens√≠veis, mascare com Faker mantendo distribuicao estatistica. Flyway e Liquibase tem validacao de migrations para Java."}
{"instruction": "Como organizar a estrutura de testes em um projeto grande?", "response": "Organize por tipo e dominio. Estrutura recomendada: tests/unit/ para testes unitarios espelhando src/, tests/integration/ para testes entre componentes, tests/e2e/ para testes de ponta a ponta, tests/performance/ para carga e stress, tests/fixtures/ para dados compartilhados, tests/conftest.py para fixtures pytest globais. Nomeie arquivos test_<modulo>.py e funcoes test_<comportamento>. Use markers pytest para categorizar: @pytest.mark.slow, @pytest.mark.integration. Configure pytest.ini com addopts para defaults. Separe requirements-test.txt. CI executa em ordem: unitarios primeiro (rapidos), integracao depois, e2e por ultimo. Paraleliza com pytest-xdist: pytest -n auto. Mantenha testes independentes, sem ordem de execucao."}
{"instruction": "Qual a estrategia para testes de feature flags e A/B testing?", "response": "Feature flags requerem testes em ambos estados. Parametrize testes: @pytest.mark.parametrize('flag_enabled', [True, False]) def test_feature(flag_enabled, mocker): mocker.patch('config.FEATURE_X', flag_enabled), ... Teste comportamento com flag on e off. Para A/B tests, valide que usuarios sao distribuidos corretamente e que cada variante funciona. Mock random para testes deterministicos. Ferramentas como LaunchDarkly ou Unleash tem SDKs de teste. Valide cleanup de flags antigas nao deixa codigo morto. Teste rollout gradual: 10%, 50%, 100%. Monitore metricas por variante em staging antes de producao. Documente hipoteses e criterios de sucesso. Testes de integracao devem cobrir caminhos com flags em diferentes estados para evitar surpresas em rollout."}
{"instruction": "Como implementar testes de resiliencia e retry logic?", "response": "Teste comportamento sob falhas. Use tenacity para retry em Python: @retry(stop=stop_after_attempt(3), wait=wait_exponential()). Teste que retries acontecem: with pytest.raises(Exception): func_that_retries(). Mock servico para falhar N vezes e depois suceder. Valide backoff exponencial com timestamps. Teste circuit breaker com pybreaker: breaker = CircuitBreaker(fail_max=3). Simule falhas consecutivas e verifique estado OPEN. Teste timeout com responses: responses.add(responses.GET, url, body=requests.exceptions.Timeout()). Valide fallbacks quando servico indisponivel. Chaos testing complementa: mate pods, introduza latencia com tc netem. Metricas de retry e fallback devem ser expostas para monitoring. Documente comportamento esperado sob falha para cada integracao externa."}
{"instruction": "Explique melhores praticas para testes em arquitetura de microservicos.", "response": "Em microservicos, cada servico tem suite de testes independente. Testes unitarios internos ao servico, mocks para dependencias externas. Contract tests entre servicos com Pact evitam integracao quebrada. Testes de integracao usam containers dos servicos dependentes ou stubs. Ambiente de staging replica producao para testes end-to-end. Service mesh como Istio permite injecao de falhas para testes de resiliencia. Cada servico tem pipeline independente, contract tests rodam em ambos lados. Teste idempotencia de mensagens em filas. Distributed tracing com Jaeger ajuda debug. Evite testes que dependem de ordem de deploy. Health checks validam dependencias no startup. Feature flags permitem deploy sem ativar funcionalidade. Monitore testes flakey e corrija imediatamente para manter confianca no pipeline."}
